{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/document_classifier_model.pkl', 'rb') as file:\n",
    "    load_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The president of the Socialist group listed the concessions obtained, and said he wanted to give “every chance to negotiation”. He warned, however, that “a vote of no confidence is possible at any time”. The vote on the motion of no confidence tabled by the “insoumis” will take place between 5:38pm and 5:58pm.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_content = []\n",
    "\n",
    "with open(f'new-test-data.txt', 'r', encoding='utf8') as file_reader:\n",
    "    for line in file_reader:\n",
    "        file_content.append(line)\n",
    "\n",
    "file_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The president of the Socialist group listed th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content\n",
       "0  The president of the Socialist group listed th..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data={'content': file_content})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###On définit une fonction qui supprime les ponctuations\n",
    "def remove_punctuation(text):\n",
    "    return ''.join([t for t in text if t not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###On définit une fonction qui applique la tokenisation sur nos données.\n",
    "def tokenizeText(text):\n",
    "    return ' '.join(word for word in re.split('\\W+', text.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_stopwords = stopwords.words('french')\n",
    "english_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###On définit une fonction qui enlève les stopwords (Anglais et Français)\n",
    "def applyStopwords(text):\n",
    "    return ' '.join(word for word in text.split() if (word not in english_stopwords) and (word not in french_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###On applique la lemmatisation sur nos données\n",
    "def applyLemmatization(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join(lemmatizer.lemmatize(word) for word in text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/count_vectorizer.pkl', 'rb') as file:\n",
    "    countVectorizer = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_file_processing(_dsf: pd.DataFrame):\n",
    "    ###Suppression des doublons\n",
    "    df.drop_duplicates(inplace = True)\n",
    "\n",
    "    ###Suppression de tous les espaces (' ', \\n, ...)\n",
    "    df['content'] = df['content'].str.strip().replace('', np.nan)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    ###Suppression des ponctuations\n",
    "    df['cleaned_content'] = df['content'].apply(lambda x: remove_punctuation(x))\n",
    "\n",
    "    ###On applique la tokenisation\n",
    "    df['cleaned_content'] = df['cleaned_content'].apply(lambda x: tokenizeText(x))\n",
    "\n",
    "    ###On applique les stopwords\n",
    "    df['cleaned_content'] = df['cleaned_content'].apply(lambda x: applyStopwords(x))\n",
    "\n",
    "    ###On applique la lemmatisation\n",
    "    df[\"cleaned_content\"] = df[\"cleaned_content\"].apply(lambda x: applyLemmatization(x))\n",
    "\n",
    "    ###On va transformer nos données (textuelles) en vecteur (utiliser transform sur les nouvelles données)\n",
    "    cleaned_content_vectorized= countVectorizer.transform(df['cleaned_content'])\n",
    "\n",
    "    return cleaned_content_vectorized.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 30546)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = input_file_processing(df)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['politics'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = load_model.predict(X)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_frequent_item(arr):\n",
    "  unique, counts = np.unique(arr, return_counts=True)\n",
    "  return unique[np.argmax(counts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'politics'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_most_frequent_item(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11773509, 0.02342258, 0.85159163, 0.00287646, 0.00437424]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_probs = load_model.predict_proba(X)\n",
    "predicted_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8515916275682073"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_probs.max()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
