{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/document_classifier_model.pkl', 'rb') as file:\n",
    "    load_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Despite qualifying for the quarter-finals of the King's Cup after beating Celta in extra time, the joy was short-lived for Real Madrid, who this morning received further bad news due to injury.\\n\",\n",
       " '\\n',\n",
       " 'On Friday January 17, the capital club announced that Eduardo Camavinga, who came off the bench during the cup match, is suffering from muscle damage to the biceps femoris in his left leg. \\n',\n",
       " '\\n',\n",
       " \"After undergoing various tests at the club's medical department, the Frenchman has been ruled out of the next few matches. The club says that his return to the squad will depend on the progress of his recovery, as is usually the case. \\n\",\n",
       " '\\n',\n",
       " \"However, initial reports suggest that he will be unavailable for around three weeks, which means that Carlo Ancelotti, on paper, will not be able to count on him for the matches against Las Palmas, Salzburg, Real Valladolid, Brest and Espanyol, casting doubt on his presence for the decisive derby against Atlético de Madrid, which takes place in 22 days' time on February 9.\\n\",\n",
       " '\\n',\n",
       " 'Translated with DeepL.com (free version)']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_content = []\n",
    "\n",
    "with open(f'new-test-data.txt', 'r', encoding='utf8') as file_reader:\n",
    "    for line in file_reader:\n",
    "        file_content.append(line)\n",
    "\n",
    "file_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Despite qualifying for the quarter-finals of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>On Friday January 17, the capital club announc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>After undergoing various tests at the club's m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>However, initial reports suggest that he will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Translated with DeepL.com (free version)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content\n",
       "0  Despite qualifying for the quarter-finals of t...\n",
       "1                                                 \\n\n",
       "2  On Friday January 17, the capital club announc...\n",
       "3                                                 \\n\n",
       "4  After undergoing various tests at the club's m...\n",
       "5                                                 \\n\n",
       "6  However, initial reports suggest that he will ...\n",
       "7                                                 \\n\n",
       "8           Translated with DeepL.com (free version)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data={'content': file_content})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "###On définit une fonction qui supprime les ponctuations\n",
    "def remove_punctuation(text):\n",
    "    return ''.join([t for t in text if t not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "###On définit une fonction qui applique la tokenisation sur nos données.\n",
    "def tokenizeText(text):\n",
    "    return ' '.join(word for word in re.split('\\W+', text.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_stopwords = stopwords.words('french')\n",
    "english_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "###On définit une fonction qui enlève les stopwords (Anglais et Français)\n",
    "def applyStopwords(text):\n",
    "    return ' '.join(word for word in text.split() if (word not in english_stopwords) and (word not in french_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "###On applique la lemmatisation sur nos données\n",
    "def applyLemmatization(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join(lemmatizer.lemmatize(word) for word in text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/count_vectorizer.pkl', 'rb') as file:\n",
    "    countVectorizer = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_file_processing(_dsf: pd.DataFrame):\n",
    "    ###Suppression des doublons\n",
    "    df.drop_duplicates(inplace = True)\n",
    "\n",
    "    ###Suppression de tous les espaces (' ', \\n, ...)\n",
    "    df['content'] = df['content'].str.strip().replace('', np.nan)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    ###Suppression des ponctuations\n",
    "    df['cleaned_content'] = df['content'].apply(lambda x: remove_punctuation(x))\n",
    "\n",
    "    ###On applique la tokenisation\n",
    "    df['cleaned_content'] = df['cleaned_content'].apply(lambda x: tokenizeText(x))\n",
    "\n",
    "    ###On applique les stopwords\n",
    "    df['cleaned_content'] = df['cleaned_content'].apply(lambda x: applyStopwords(x))\n",
    "\n",
    "    ###On applique la lemmatisation\n",
    "    df[\"cleaned_content\"] = df[\"cleaned_content\"].apply(lambda x: applyLemmatization(x))\n",
    "\n",
    "    ###On va transformer nos données (textuelles) en vecteur (utiliser transform sur les nouvelles données)\n",
    "    cleaned_content_vectorized= countVectorizer.transform(df['cleaned_content'])\n",
    "\n",
    "    return cleaned_content_vectorized.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 30546)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = input_file_processing(df)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sport', 'sport', 'sport', 'sport', 'tech'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = load_model.predict(X)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_frequent_item(arr):\n",
    "  unique, counts = np.unique(arr, return_counts=True)\n",
    "  return unique[np.argmax(counts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sport'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_most_frequent_item(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.01184967e-03, 9.82702233e-04, 3.78131732e-04, 9.95443769e-01,\n",
       "        2.18354743e-03],\n",
       "       [9.00719585e-04, 7.98508178e-04, 3.40100369e-04, 9.96320184e-01,\n",
       "        1.64048818e-03],\n",
       "       [1.44184178e-03, 1.18641039e-03, 5.57441767e-04, 9.95445558e-01,\n",
       "        1.36874760e-03],\n",
       "       [3.15619943e-03, 3.82414258e-03, 1.27704289e-03, 9.83699245e-01,\n",
       "        8.04337046e-03],\n",
       "       [3.40176836e-02, 1.75985646e-01, 3.44162389e-02, 2.18118851e-02,\n",
       "        7.33768546e-01]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_probs = load_model.predict_proba(X)\n",
    "predicted_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9963201836864433"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_probs.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# def generer_titre_intelligent(contenu):\n",
    "#     # Charge un modèle pré-entraîné pour la génération de texte\n",
    "#     generer_titre = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "#     # Génére un résumé du contenu, qui servira de titre\n",
    "#     resume = generer_titre(contenu, max_length=60, min_length=10, do_sample=False)\n",
    "#     return resume[0]['summary_text']\n",
    "\n",
    "# # Lis le contenu du fichier (remplace le chemin par celui de ton fichier)\n",
    "# with open('new-test-data.txt', 'r', encoding='utf-8') as f:\n",
    "#     contenu = f.read()\n",
    "\n",
    "# titre = generer_titre_intelligent(contenu)\n",
    "# print(f\"Titre généré : {titre}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
